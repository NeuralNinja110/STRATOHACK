{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26a34b2-533c-4dbf-94a5-290334dfc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Machine Learning Model for Airline Price Prediction \n",
    "by using xgboost\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import joblib\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad26f337-2973-4a54-8b01-b482e2974556",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9957f3-2e35-4911-88b3-39e03f0428eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostAirlinePredictor:\n",
    "    \"\"\"Simplified XGBoost model for airline price prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "                 subsample=0.8, colsample_bytree=0.8, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize XGBoost model with parameters\n",
    "        \n",
    "        Args:\n",
    "            n_estimators: Number of boosting rounds\n",
    "            max_depth: Maximum tree depth\n",
    "            learning_rate: Learning rate\n",
    "            subsample: Subsample ratio\n",
    "            colsample_bytree: Feature subsample ratio\n",
    "            random_state: Random state for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.model = None\n",
    "        self.is_trained = False\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, early_stopping_rounds=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the XGBoost model\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training target\n",
    "            X_val: Validation features (optional)\n",
    "            y_val: Validation target (optional)\n",
    "            early_stopping_rounds: Early stopping rounds\n",
    "            verbose: Whether to print training progress\n",
    "        \"\"\"\n",
    "        logger.info(\"Training XGBoost model...\")\n",
    "        \n",
    "        # Store feature names\n",
    "        if hasattr(X_train, 'columns'):\n",
    "            self.feature_names = list(X_train.columns)\n",
    "        else:\n",
    "            self.feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "        \n",
    "        # Initialize XGBoost model\n",
    "        self.model = xgb.XGBRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            learning_rate=self.learning_rate,\n",
    "            subsample=self.subsample,\n",
    "            colsample_bytree=self.colsample_bytree,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1,\n",
    "            device='cpu' \n",
    "        )\n",
    "        \n",
    "        # Prepare evaluation set for early stopping\n",
    "        eval_set = None\n",
    "        if X_val is not None and y_val is not None:\n",
    "            eval_set = [(X_val, y_val)]\n",
    "        \n",
    "        # Train the model - XGBoost 3.0+ compatible\n",
    "        if eval_set and early_stopping_rounds:\n",
    "            # XGBoost 3.0+ syntax\n",
    "            self.model.set_params(early_stopping_rounds=early_stopping_rounds)\n",
    "            self.model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=eval_set,\n",
    "                verbose=verbose\n",
    "            )\n",
    "        else:\n",
    "            # Simple training without early stopping\n",
    "            self.model.fit(X_train, y_train, verbose=verbose)\n",
    "        \n",
    "        self.is_trained = True\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_pred = self.model.predict(X_train)\n",
    "        self.metrics['train'] = self._calculate_metrics(y_train, train_pred)\n",
    "        \n",
    "        # Calculate validation metrics if provided\n",
    "        if X_val is not None and y_val is not None:\n",
    "            val_pred = self.model.predict(X_val)\n",
    "            self.metrics['val'] = self._calculate_metrics(y_val, val_pred)\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"Training completed!\")\n",
    "            print(f\"Training RMSE: {self.metrics['train']['rmse']:.2f}\")\n",
    "            if 'val' in self.metrics:\n",
    "                print(f\"Validation RMSE: {self.metrics['val']['rmse']:.2f}\")\n",
    "        \n",
    "        logger.info(\"XGBoost training completed\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model is not trained yet. Call train() first.\")\n",
    "        \n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate evaluation metrics\"\"\"\n",
    "        return {\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'mse': mean_squared_error(y_true, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'r2': r2_score(y_true, y_pred),\n",
    "            'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate model performance on given dataset\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model is not trained yet.\")\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        metrics = self._calculate_metrics(y, y_pred)\n",
    "        \n",
    "        print(\"Model Performance:\")\n",
    "        print(f\"MAE: {metrics['mae']:.2f}\")\n",
    "        print(f\"RMSE: {metrics['rmse']:.2f}\")\n",
    "        print(f\"R²: {metrics['r2']:.4f}\")\n",
    "        print(f\"MAPE: {metrics['mape']:.2f}%\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def get_feature_importance(self, plot=True, top_n=15):\n",
    "        \"\"\"\n",
    "        Get and visualize feature importance\n",
    "        \n",
    "        Args:\n",
    "            plot: Whether to create a plot\n",
    "            top_n: Number of top features to show in plot\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model is not trained yet.\")\n",
    "        \n",
    "        # Create feature importance dataframe\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': self.feature_names,\n",
    "            'importance': self.model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        if plot:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            top_features = importance_df.head(top_n)\n",
    "            \n",
    "            plt.barh(range(len(top_features)), top_features['importance'])\n",
    "            plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title(f'Top {top_n} Feature Importance - XGBoost')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    def plot_predictions(self, X, y, title=\"XGBoost Predictions vs Actual\"):\n",
    "        \"\"\"Plot predictions vs actual values\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model is not trained yet.\")\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Scatter plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(y, y_pred, alpha=0.5)\n",
    "        plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Actual Prices')\n",
    "        plt.ylabel('Predicted Prices')\n",
    "        plt.title('Predicted vs Actual')\n",
    "        \n",
    "        # Residuals plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        residuals = y - y_pred\n",
    "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel('Predicted Prices')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.title('Residuals Plot')\n",
    "        \n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print metrics\n",
    "        metrics = self._calculate_metrics(y, y_pred)\n",
    "        print(f\"R² Score: {metrics['r2']:.4f}\")\n",
    "        print(f\"RMSE: {metrics['rmse']:.2f}\")\n",
    "        \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model is not trained yet.\")\n",
    "        \n",
    "        Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        model_data = {\n",
    "            'model': self.model,\n",
    "            'feature_names': self.feature_names,\n",
    "            'metrics': self.metrics,\n",
    "            'is_trained': self.is_trained,\n",
    "            'params': {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'max_depth': self.max_depth,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'subsample': self.subsample,\n",
    "                'colsample_bytree': self.colsample_bytree,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, filepath)\n",
    "        logger.info(f\"Model saved to {filepath}\")\n",
    "        print(f\"Model saved successfully to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model\"\"\"\n",
    "        model_data = joblib.load(filepath)\n",
    "        \n",
    "        self.model = model_data['model']\n",
    "        self.feature_names = model_data['feature_names']\n",
    "        self.metrics = model_data['metrics']\n",
    "        self.is_trained = model_data['is_trained']\n",
    "        \n",
    "        # Load parameters\n",
    "        params = model_data['params']\n",
    "        self.n_estimators = params['n_estimators']\n",
    "        self.max_depth = params['max_depth']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.subsample = params['subsample']\n",
    "        self.colsample_bytree = params['colsample_bytree']\n",
    "        self.random_state = params['random_state']\n",
    "        \n",
    "        logger.info(f\"Model loaded from {filepath}\")\n",
    "        print(f\"Model loaded successfully from {filepath}\")\n",
    "    \n",
    "    def hyperparameter_tuning(self, X_train, y_train, X_val, y_val, param_grid=None):\n",
    "        \"\"\"\n",
    "        Simple hyperparameter tuning using validation set\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training target\n",
    "            X_val: Validation features\n",
    "            y_val: Validation target\n",
    "            param_grid: Dictionary of parameters to tune\n",
    "        \"\"\"\n",
    "        if param_grid is None:\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "            }\n",
    "        \n",
    "        best_score = float('inf')\n",
    "        best_params = {}\n",
    "        results = []\n",
    "        \n",
    "        print(\"Starting hyperparameter tuning...\")\n",
    "        \n",
    "        # Simple grid search\n",
    "        from itertools import product\n",
    "        \n",
    "        param_names = list(param_grid.keys())\n",
    "        param_values = list(param_grid.values())\n",
    "        \n",
    "        total_combinations = np.prod([len(v) for v in param_values])\n",
    "        print(f\"Total combinations to test: {total_combinations}\")\n",
    "        \n",
    "        for i, combination in enumerate(product(*param_values)):\n",
    "            params = dict(zip(param_names, combination))\n",
    "            \n",
    "            # Create temporary model with current parameters\n",
    "            temp_model = XGBoostAirlinePredictor(**params, random_state=self.random_state)\n",
    "            temp_model.train(X_train, y_train, X_val, y_val, verbose=False)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_pred = temp_model.predict(X_val)\n",
    "            val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "            \n",
    "            results.append({\n",
    "                'params': params.copy(),\n",
    "                'val_rmse': val_rmse\n",
    "            })\n",
    "            \n",
    "            if val_rmse < best_score:\n",
    "                best_score = val_rmse\n",
    "                best_params = params.copy()\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Tested {i + 1}/{total_combinations} combinations. Best RMSE so far: {best_score:.4f}\")\n",
    "        \n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best validation RMSE: {best_score:.4f}\")\n",
    "        \n",
    "        # Update model with best parameters\n",
    "        self.__init__(**best_params, random_state=self.random_state)\n",
    "        \n",
    "        return best_params, best_score, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68834a16-8475-41ef-b77f-922010635c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training XGBoost model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sample data for testing...\n",
      "Initializing XGBoost model...\n",
      "Training model...\n",
      "[0]\tvalidation_0-rmse:109.14125\n",
      "[1]\tvalidation_0-rmse:100.58039\n",
      "[2]\tvalidation_0-rmse:98.31126\n",
      "[3]\tvalidation_0-rmse:90.94879\n",
      "[4]\tvalidation_0-rmse:84.73283\n",
      "[5]\tvalidation_0-rmse:83.33077\n",
      "[6]\tvalidation_0-rmse:77.52579\n",
      "[7]\tvalidation_0-rmse:73.43990\n",
      "[8]\tvalidation_0-rmse:69.40930\n",
      "[9]\tvalidation_0-rmse:67.90851\n",
      "[10]\tvalidation_0-rmse:64.12301\n",
      "[11]\tvalidation_0-rmse:60.85854\n",
      "[12]\tvalidation_0-rmse:58.13647\n",
      "[13]\tvalidation_0-rmse:55.60187\n",
      "[14]\tvalidation_0-rmse:53.19185\n",
      "[15]\tvalidation_0-rmse:51.19771\n",
      "[16]\tvalidation_0-rmse:49.33250\n",
      "[17]\tvalidation_0-rmse:47.80231\n",
      "[18]\tvalidation_0-rmse:46.34741\n",
      "[19]\tvalidation_0-rmse:44.79926\n",
      "[20]\tvalidation_0-rmse:43.63541\n",
      "[21]\tvalidation_0-rmse:42.58767\n",
      "[22]\tvalidation_0-rmse:41.52879\n",
      "[23]\tvalidation_0-rmse:41.44672\n",
      "[24]\tvalidation_0-rmse:40.41998\n",
      "[25]\tvalidation_0-rmse:39.73207\n",
      "[26]\tvalidation_0-rmse:39.20685\n",
      "[27]\tvalidation_0-rmse:38.95698\n",
      "[28]\tvalidation_0-rmse:38.38058\n",
      "[29]\tvalidation_0-rmse:37.68041\n",
      "[30]\tvalidation_0-rmse:37.26872\n",
      "[31]\tvalidation_0-rmse:37.03864\n",
      "[32]\tvalidation_0-rmse:36.78330\n",
      "[33]\tvalidation_0-rmse:36.43971\n",
      "[34]\tvalidation_0-rmse:35.99037\n",
      "[35]\tvalidation_0-rmse:35.69677\n",
      "[36]\tvalidation_0-rmse:35.44147\n",
      "[37]\tvalidation_0-rmse:35.37315\n",
      "[38]\tvalidation_0-rmse:35.18197\n",
      "[39]\tvalidation_0-rmse:35.11191\n",
      "[40]\tvalidation_0-rmse:35.02405\n",
      "[41]\tvalidation_0-rmse:34.75949\n",
      "[42]\tvalidation_0-rmse:34.67393\n",
      "[43]\tvalidation_0-rmse:34.58652\n",
      "[44]\tvalidation_0-rmse:34.51904\n",
      "[45]\tvalidation_0-rmse:34.46516\n",
      "[46]\tvalidation_0-rmse:34.42235\n",
      "[47]\tvalidation_0-rmse:34.43057\n",
      "[48]\tvalidation_0-rmse:34.30603\n",
      "[49]\tvalidation_0-rmse:34.26169\n",
      "[50]\tvalidation_0-rmse:34.19884\n",
      "[51]\tvalidation_0-rmse:34.13276\n",
      "[52]\tvalidation_0-rmse:34.04906\n",
      "[53]\tvalidation_0-rmse:33.93379\n",
      "[54]\tvalidation_0-rmse:33.89111\n",
      "[55]\tvalidation_0-rmse:33.87794\n",
      "[56]\tvalidation_0-rmse:33.86586\n",
      "[57]\tvalidation_0-rmse:33.80993\n",
      "[58]\tvalidation_0-rmse:33.76155\n",
      "[59]\tvalidation_0-rmse:33.78044\n",
      "[60]\tvalidation_0-rmse:33.76071\n",
      "[61]\tvalidation_0-rmse:33.69521\n",
      "[62]\tvalidation_0-rmse:33.64181\n",
      "[63]\tvalidation_0-rmse:33.62659\n",
      "[64]\tvalidation_0-rmse:33.55801\n",
      "[65]\tvalidation_0-rmse:33.48182\n",
      "[66]\tvalidation_0-rmse:33.44638\n",
      "[67]\tvalidation_0-rmse:33.42872\n",
      "[68]\tvalidation_0-rmse:33.40152\n",
      "[69]\tvalidation_0-rmse:33.40737\n",
      "[70]\tvalidation_0-rmse:33.43532\n",
      "[71]\tvalidation_0-rmse:33.41314\n",
      "[72]\tvalidation_0-rmse:33.41209\n",
      "[73]\tvalidation_0-rmse:33.36736\n",
      "[74]\tvalidation_0-rmse:33.32238\n",
      "[75]\tvalidation_0-rmse:33.28478\n",
      "[76]\tvalidation_0-rmse:33.29882\n",
      "[77]\tvalidation_0-rmse:33.28561\n",
      "[78]\tvalidation_0-rmse:33.26234\n",
      "[79]\tvalidation_0-rmse:33.27451\n",
      "[80]\tvalidation_0-rmse:33.27375\n",
      "[81]\tvalidation_0-rmse:33.28159\n",
      "[82]\tvalidation_0-rmse:33.28001\n",
      "[83]\tvalidation_0-rmse:33.29496\n",
      "[84]\tvalidation_0-rmse:33.25349\n",
      "[85]\tvalidation_0-rmse:33.26354\n",
      "[86]\tvalidation_0-rmse:33.23147\n",
      "[87]\tvalidation_0-rmse:33.22848\n",
      "[88]\tvalidation_0-rmse:33.22097\n",
      "[89]\tvalidation_0-rmse:33.23945\n",
      "[90]\tvalidation_0-rmse:33.24046\n",
      "[91]\tvalidation_0-rmse:33.28884\n",
      "[92]\tvalidation_0-rmse:33.30701\n",
      "[93]\tvalidation_0-rmse:33.30766\n",
      "[94]\tvalidation_0-rmse:33.32800\n",
      "[95]\tvalidation_0-rmse:33.33077\n",
      "[96]\tvalidation_0-rmse:33.32602\n",
      "[97]\tvalidation_0-rmse:33.31509\n",
      "[98]\tvalidation_0-rmse:33.30972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:XGBoost training completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Training RMSE: 4.90\n",
      "Validation RMSE: 33.22\n",
      "\n",
      "Evaluating on test set:\n",
      "Model Performance:\n",
      "MAE: 25.70\n",
      "RMSE: 31.35\n",
      "R²: 0.9364\n",
      "MAPE: 13.83%\n",
      "\n",
      "Top 10 Feature Importance:\n",
      "       feature  importance\n",
      "0    feature_0    0.549522\n",
      "1    feature_1    0.133193\n",
      "2    feature_2    0.057326\n",
      "3    feature_3    0.050080\n",
      "12  feature_12    0.026344\n",
      "11  feature_11    0.025024\n",
      "14  feature_14    0.024223\n",
      "6    feature_6    0.023095\n",
      "5    feature_5    0.020662\n",
      "4    feature_4    0.020371\n",
      "\n",
      "Testing save/load functionality...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model saved to test_xgb_model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to test_xgb_model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model loaded from test_xgb_model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from test_xgb_model.pkl\n",
      "Sample predictions: [243.79791 372.4136  360.92062 243.52837 427.9935 ]\n",
      "\n",
      "XGBoost model test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create sample data for testing\n",
    "    print(\"Creating sample data for testing...\")\n",
    "    \n",
    "    # Generate sample dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    n_features = 15\n",
    "    \n",
    "    X_sample = np.random.randn(n_samples, n_features)\n",
    "    # Create realistic price relationships\n",
    "    y_sample = (X_sample[:, 0] * 100 +  # route factor\n",
    "                X_sample[:, 1] * 50 +   # seasonality\n",
    "                X_sample[:, 2] * 30 +   # fuel prices\n",
    "                X_sample[:, 3] * 20 +   # holidays\n",
    "                np.random.normal(0, 20, n_samples) + 300)  # base price + noise\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "    X_sample = pd.DataFrame(X_sample, columns=feature_names)\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(0.7 * len(X_sample))\n",
    "    val_idx = int(0.85 * len(X_sample))\n",
    "    \n",
    "    X_train = X_sample[:split_idx]\n",
    "    y_train = y_sample[:split_idx]\n",
    "    X_val = X_sample[split_idx:val_idx]\n",
    "    y_val = y_sample[split_idx:val_idx]\n",
    "    X_test = X_sample[val_idx:]\n",
    "    y_test = y_sample[val_idx:]\n",
    "    \n",
    "    # Initialize and train model\n",
    "    print(\"Initializing XGBoost model...\")\n",
    "    model = XGBoostAirlinePredictor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    model.train(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set:\")\n",
    "    test_metrics = model.evaluate(X_test, y_test)\n",
    "        \n",
    "    # Test save/load\n",
    "    print(\"\\nTesting save/load functionality...\")\n",
    "    model.save_model(\"test_xgb_model.pkl\")\n",
    "    \n",
    "    # Create new model and load\n",
    "    new_model = XGBoostAirlinePredictor()\n",
    "    new_model.load_model(\"test_xgb_model.pkl\")\n",
    "    \n",
    "    # Test prediction with loaded model\n",
    "    test_pred = new_model.predict(X_test[:5])\n",
    "    print(f\"Sample predictions: {test_pred}\")\n",
    "    \n",
    "    print(\"\\nXGBoost model test completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a47638-5db9-45d6-9e7e-895218a797ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
