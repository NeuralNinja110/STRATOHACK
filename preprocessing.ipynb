{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b53205f3-65df-4104-b6c4-1827b671c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data preprocessing pipeline for airline price prediction\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import logging\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a222d12-2a65-46e1-bcbf-a27289fe110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Collecting all external data...\n",
      "INFO:__main__:Loading cached fuel data\n",
      "INFO:__main__:Loading cached holiday data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1096 entries, 0 to 1095\n",
      "Data columns (total 4 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   date                       1096 non-null   object \n",
      " 1   fuel_price_usd_per_barrel  1096 non-null   float64\n",
      " 2   fuel_price_inr_per_liter   1096 non-null   float64\n",
      " 3   price_change_pct           1096 non-null   float64\n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 34.4+ KB\n",
      "Fuel data shape: (1096, 4)\n",
      "Holiday data shape: (69, 6)\n",
      "\n",
      "Fuel data sample:\n",
      "        date  fuel_price_usd_per_barrel  fuel_price_inr_per_liter  \\\n",
      "0 2022-01-01                     101.17                     53.62   \n",
      "1 2022-01-02                     100.08                     53.04   \n",
      "2 2022-01-03                     101.84                     53.97   \n",
      "3 2022-01-04                     103.79                     55.01   \n",
      "4 2022-01-05                     100.43                     53.23   \n",
      "\n",
      "   price_change_pct  \n",
      "0              0.99  \n",
      "1             -0.28  \n",
      "2              1.30  \n",
      "3              3.05  \n",
      "4             -0.47  \n",
      "\n",
      "Holiday data sample:\n",
      "         date      holiday_name  is_national_holiday holiday_type  year  \\\n",
      "0  2022-01-26      Republic Day                 True     National  2022   \n",
      "1  2022-08-15  Independence Day                 True     National  2022   \n",
      "2  2022-10-02    Gandhi Jayanti                 True     National  2022   \n",
      "3  2022-05-16    Buddha Purnima                 True     National  2022   \n",
      "4  2022-10-24            Diwali                 True     National  2022   \n",
      "\n",
      "   holiday_impact_score  \n",
      "0                   3.0  \n",
      "1                   3.0  \n",
      "2                   3.0  \n",
      "3                   3.0  \n",
      "4                   5.0  \n"
     ]
    }
   ],
   "source": [
    "%run data_acquisition.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86246923-8f14-499a-9e5a-4ad693b1d998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Collecting all external data...\n",
      "INFO:__main__:Loading cached fuel data\n",
      "INFO:__main__:Loading cached holiday data\n"
     ]
    }
   ],
   "source": [
    "manager = DataAcquisitionManager()\n",
    "external_data = manager.get_all_external_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23b5770d-c696-4d19-b771-c4dcc053dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c937ae7-d4ae-4a42-9e58-b75b8f93f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlightDataPreprocessor:\n",
    "    \"\"\"Comprehensive flight data preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"data/processed\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.label_encoders = {}\n",
    "        self.scaler = None\n",
    "        self.feature_columns = []\n",
    "        self.target_column = 'price'\n",
    "        \n",
    "        self.data_manager = DataAcquisitionManager(cache_dir)\n",
    "        \n",
    "    def load_airline_data(self, data_path: str = \"dataset\") -> pd.DataFrame:\n",
    "        \"\"\"Load and combine all airline datasets\"\"\"\n",
    "        logger.info(\"Loading airline datasets...\")\n",
    "        \n",
    "        # Load all three datasets\n",
    "        clean_df = pd.read_csv(f\"{data_path}/Clean_Dataset.csv\")\n",
    "        business_df = pd.read_csv(f\"{data_path}/business.csv\")\n",
    "        economy_df = pd.read_csv(f\"{data_path}/economy.csv\")\n",
    "        \n",
    "        logger.info(f\"Loaded datasets - Clean: {clean_df.shape}, Business: {business_df.shape}, Economy: {economy_df.shape}\")\n",
    "        \n",
    "        # Standardize the datasets\n",
    "        clean_df = self._standardize_clean_dataset(clean_df)\n",
    "        business_df = self._standardize_business_dataset(business_df)\n",
    "        economy_df = self._standardize_economy_dataset(economy_df)\n",
    "        \n",
    "        # Combine all datasets\n",
    "        combined_df = pd.concat([clean_df, business_df, economy_df], ignore_index=True)\n",
    "        \n",
    "        logger.info(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "        return combined_df\n",
    "    \n",
    "    def _standardize_clean_dataset(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Standardize the clean dataset format\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Create date column (assuming this is recent data)\n",
    "        if 'days_left' in df.columns:\n",
    "            base_date = datetime(2024, 1, 1)\n",
    "            df['date'] = base_date + pd.to_timedelta(df['days_left'], unit='D')\n",
    "        else:\n",
    "            df['date'] = datetime(2024, 1, 1)  # Default date\n",
    "            \n",
    "        # Standardize column names\n",
    "        column_mapping = {\n",
    "            'source_city': 'from',\n",
    "            'destination_city': 'to',\n",
    "            'departure_time': 'dep_time',\n",
    "            'arrival_time': 'arr_time',\n",
    "            'stops': 'stop'\n",
    "        }\n",
    "        \n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Add missing columns with defaults\n",
    "        if 'ch_code' not in df.columns:\n",
    "            df['ch_code'] = df['airline'].str[:2]\n",
    "        if 'num_code' not in df.columns:\n",
    "            df['num_code'] = df['flight'].str.extract(r'(\\d+)').fillna('000')\n",
    "        if 'time_taken' not in df.columns:\n",
    "            df['time_taken'] = df['duration'].apply(self._convert_duration_to_standard)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def _standardize_business_dataset(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Standardize the business dataset format\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Convert date format\n",
    "        if 'ggldate' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['ggldate'], format='%d-%m-%Y', errors='coerce')\n",
    "        \n",
    "        # Clean price column\n",
    "        if 'price' in df.columns:\n",
    "            df['price'] = df['price'].astype(str).str.replace(',', '').str.replace('\"', '')\n",
    "            df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "        \n",
    "        # Add class column\n",
    "        df['class'] = 'Business'\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _standardize_economy_dataset(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Standardize the economy dataset format\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Convert date format\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y', errors='coerce')\n",
    "        \n",
    "        # Clean price column\n",
    "        if 'price' in df.columns:\n",
    "            df['price'] = df['price'].astype(str).str.replace(',', '').str.replace('\"', '')\n",
    "            df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "        \n",
    "        # Add class column if not present\n",
    "        if 'class' not in df.columns:\n",
    "            df['class'] = 'Economy'\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _convert_duration_to_standard(self, duration) -> str:\n",
    "        \"\"\"Convert duration to standard format (Xh Ym)\"\"\"\n",
    "        if pd.isna(duration):\n",
    "            return \"2h 00m\"\n",
    "        \n",
    "        duration_str = str(duration)\n",
    "        \n",
    "        # If already in decimal format (like 2.17), convert to hours and minutes\n",
    "        try:\n",
    "            decimal_hours = float(duration_str)\n",
    "            hours = int(decimal_hours)\n",
    "            minutes = int((decimal_hours - hours) * 60)\n",
    "            return f\"{hours}h {minutes:02d}m\"\n",
    "        except:\n",
    "            # If already in standard format, return as is\n",
    "            return duration_str\n",
    "    \n",
    "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Comprehensive data cleaning\"\"\"\n",
    "        logger.info(\"Starting data cleaning...\")\n",
    "        \n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Remove duplicates\n",
    "        initial_shape = df_clean.shape\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        logger.info(f\"Removed {initial_shape[0] - df_clean.shape[0]} duplicate rows\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        df_clean = self._handle_missing_values(df_clean)\n",
    "        \n",
    "        # Clean price column\n",
    "        df_clean = self._clean_price_column(df_clean)\n",
    "        \n",
    "        # Clean time columns\n",
    "        df_clean = self._clean_time_columns(df_clean)\n",
    "        \n",
    "        # Remove outliers\n",
    "        df_clean = self._remove_outliers(df_clean)\n",
    "        \n",
    "        logger.info(f\"Data cleaning completed. Final shape: {df_clean.shape}\")\n",
    "        return df_clean\n",
    "    \n",
    "    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handle missing values in the dataset\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Fill missing categorical values\n",
    "        categorical_columns = ['airline', 'from', 'to', 'class', 'stop']\n",
    "        for col in categorical_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')\n",
    "        \n",
    "        # Fill missing numerical values\n",
    "        numerical_columns = ['price', 'duration']\n",
    "        for col in numerical_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        # Fill missing time values\n",
    "        time_columns = ['dep_time', 'arr_time']\n",
    "        for col in time_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('Unknown')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _clean_price_column(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean the price column\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if 'price' in df.columns:\n",
    "            # Remove commas, quotes, and currency symbols\n",
    "            df['price'] = df['price'].astype(str)\n",
    "            df['price'] = df['price'].str.replace(r'[^\\d.]', '', regex=True)\n",
    "            df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "            \n",
    "            # Remove rows with invalid prices\n",
    "            df = df[df['price'] > 0]\n",
    "            \n",
    "            # Convert to INR if needed (assuming some prices might be in other currencies)\n",
    "            # This is a simple heuristic - prices less than 1000 might be in thousands\n",
    "            df.loc[df['price'] < 1000, 'price'] *= 1000\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _clean_time_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean time-related columns\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Standardize time format\n",
    "        time_mappings = {\n",
    "            'Early_Morning': 'Early Morning',\n",
    "            'Late_Night': 'Night',\n",
    "            'early_morning': 'Early Morning',\n",
    "            'late_night': 'Night'\n",
    "        }\n",
    "        \n",
    "        for col in ['dep_time', 'arr_time']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].replace(time_mappings)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _remove_outliers(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Remove outliers using IQR method\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if 'price' in df.columns:\n",
    "            Q1 = df['price'].quantile(0.25)\n",
    "            Q3 = df['price'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            initial_count = len(df)\n",
    "            df = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]\n",
    "            logger.info(f\"Removed {initial_count - len(df)} price outliers\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Comprehensive feature engineering\"\"\"\n",
    "        logger.info(\"Starting feature engineering...\")\n",
    "        \n",
    "        df_features = df.copy()\n",
    "        \n",
    "        # Temporal features\n",
    "        df_features = self._create_temporal_features(df_features)\n",
    "        \n",
    "        # Route features\n",
    "        df_features = self._create_route_features(df_features)\n",
    "        \n",
    "        # Time-based features\n",
    "        df_features = self._create_time_features(df_features)\n",
    "        \n",
    "        # Duration features\n",
    "        df_features = self._create_duration_features(df_features)\n",
    "        \n",
    "        # Airline features\n",
    "        df_features = self._create_airline_features(df_features)\n",
    "        \n",
    "        # External data features (fuel prices, holidays)\n",
    "        df_features = self._add_external_features(df_features)\n",
    "        \n",
    "        logger.info(f\"Feature engineering completed. New shape: {df_features.shape}\")\n",
    "        return df_features\n",
    "    \n",
    "    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create temporal features from date\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            df['year'] = df['date'].dt.year\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df['quarter'] = df['date'].dt.quarter\n",
    "            df['day'] = df['date'].dt.day\n",
    "            df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            df['day_of_year'] = df['date'].dt.dayofyear\n",
    "            df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "            \n",
    "            # Seasonal features\n",
    "            df['season'] = df['month'].map({\n",
    "                12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "                3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "                6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "                9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
    "            })\n",
    "            \n",
    "            # Weekend indicator\n",
    "            df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "            \n",
    "            # Month-end indicator\n",
    "            df['is_month_end'] = (df['day'] >= 25).astype(int)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def _create_route_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create route-based features\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if 'from' in df.columns and 'to' in df.columns:\n",
    "            # Route popularity\n",
    "            route_counts = df.groupby(['from', 'to']).size()\n",
    "            df['route_popularity'] = df.apply(lambda x: route_counts.get((x['from'], x['to']), 0), axis=1)\n",
    "            \n",
    "            # Major cities indicator\n",
    "            major_cities = ['Delhi', 'Mumbai', 'Bangalore', 'Chennai', 'Kolkata', 'Hyderabad']\n",
    "            df['from_major_city'] = df['from'].isin(major_cities).astype(int)\n",
    "            df['to_major_city'] = df['to'].isin(major_cities).astype(int)\n",
    "            df['major_to_major'] = (df['from_major_city'] & df['to_major_city']).astype(int)\n",
    "            \n",
    "            # Route type\n",
    "            df['route_type'] = 'Domestic'  # Assuming all routes are domestic\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create time-based features\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Time slot encoding\n",
    "        time_slot_mapping = {\n",
    "            'Early Morning': 1, 'Morning': 2, 'Afternoon': 3, \n",
    "            'Evening': 4, 'Night': 5, 'Late Night': 6\n",
    "        }\n",
    "        \n",
    "        if 'dep_time' in df.columns:\n",
    "            df['dep_time_slot'] = df['dep_time'].map(time_slot_mapping).fillna(3)\n",
    "        \n",
    "        if 'arr_time' in df.columns:\n",
    "            df['arr_time_slot'] = df['arr_time'].map(time_slot_mapping).fillna(3)\n",
    "        \n",
    "        # Peak time indicators\n",
    "        peak_times = ['Morning', 'Evening']\n",
    "        if 'dep_time' in df.columns:\n",
    "            df['is_peak_departure'] = df['dep_time'].isin(peak_times).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _create_duration_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create duration-based features\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if 'time_taken' in df.columns:\n",
    "            # Extract hours and minutes from duration\n",
    "            duration_pattern = r'(\\d+)h\\s*(\\d+)m'\n",
    "            duration_matches = df['time_taken'].str.extract(duration_pattern)\n",
    "            \n",
    "            df['flight_hours'] = pd.to_numeric(duration_matches[0], errors='coerce').fillna(2)\n",
    "            df['flight_minutes'] = pd.to_numeric(duration_matches[1], errors='coerce').fillna(0)\n",
    "            df['total_flight_minutes'] = df['flight_hours'] * 60 + df['flight_minutes']\n",
    "            \n",
    "            # Duration categories\n",
    "            df['duration_category'] = pd.cut(df['total_flight_minutes'], \n",
    "                                           bins=[0, 120, 240, 480, float('inf')],\n",
    "                                           labels=['Short', 'Medium', 'Long', 'Very Long'])\n",
    "        \n",
    "        elif 'duration' in df.columns:\n",
    "            # Handle decimal duration format\n",
    "            df['flight_hours'] = df['duration'].fillna(2)\n",
    "            df['total_flight_minutes'] = df['flight_hours'] * 60\n",
    "            df['duration_category'] = pd.cut(df['total_flight_minutes'], \n",
    "                                           bins=[0, 120, 240, 480, float('inf')],\n",
    "                                           labels=['Short', 'Medium', 'Long', 'Very Long'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _create_airline_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create airline-based features\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if 'airline' in df.columns:\n",
    "            # Airline type categorization\n",
    "            budget_airlines = ['SpiceJet', 'IndiGo', 'GO_FIRST', 'AirAsia']\n",
    "            premium_airlines = ['Vistara', 'Air India']\n",
    "            \n",
    "            df['airline_type'] = 'Other'\n",
    "            df.loc[df['airline'].isin(budget_airlines), 'airline_type'] = 'Budget'\n",
    "            df.loc[df['airline'].isin(premium_airlines), 'airline_type'] = 'Premium'\n",
    "            \n",
    "            # Airline market share (based on frequency in dataset)\n",
    "            airline_counts = df['airline'].value_counts()\n",
    "            df['airline_market_share'] = df['airline'].map(airline_counts)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _add_external_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add external features (fuel prices, holidays)\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if 'date' in df.columns:\n",
    "            logger.info(\"Adding external features...\")\n",
    "            df = self.data_manager.enrich_flight_data(df, 'date')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def encode_categorical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Encode categorical features\"\"\"\n",
    "        logger.info(\"Encoding categorical features...\")\n",
    "        \n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        categorical_columns = [\n",
    "            'airline', 'from', 'to', 'class', 'stop', 'dep_time', 'arr_time',\n",
    "            'season', 'duration_category', 'airline_type', 'route_type'\n",
    "        ]\n",
    "        \n",
    "        for col in categorical_columns:\n",
    "            if col in df_encoded.columns:\n",
    "                if col not in self.label_encoders:\n",
    "                    self.label_encoders[col] = LabelEncoder()\n",
    "                    df_encoded[col + '_encoded'] = self.label_encoders[col].fit_transform(df_encoded[col].astype(str))\n",
    "                else:\n",
    "                    # Handle unseen categories\n",
    "                    unique_values = df_encoded[col].astype(str).unique()\n",
    "                    known_values = set(self.label_encoders[col].classes_)\n",
    "                    \n",
    "                    for value in unique_values:\n",
    "                        if value not in known_values:\n",
    "                            # Add new category\n",
    "                            self.label_encoders[col].classes_ = np.append(self.label_encoders[col].classes_, value)\n",
    "                    \n",
    "                    df_encoded[col + '_encoded'] = self.label_encoders[col].transform(df_encoded[col].astype(str))\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def prepare_features_for_modeling(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "        \"\"\"Prepare final feature set for modeling\"\"\"\n",
    "        logger.info(\"Preparing features for modeling...\")\n",
    "        \n",
    "        # Select features for modeling\n",
    "        feature_columns = []\n",
    "        \n",
    "        # Numerical features\n",
    "        numerical_features = [\n",
    "            'year', 'month', 'quarter', 'day', 'day_of_week', 'day_of_year', 'week_of_year',\n",
    "            'is_weekend', 'is_month_end', 'route_popularity', 'from_major_city', 'to_major_city',\n",
    "            'major_to_major', 'dep_time_slot', 'arr_time_slot', 'is_peak_departure',\n",
    "            'flight_hours', 'flight_minutes', 'total_flight_minutes', 'airline_market_share',\n",
    "            'fuel_price_usd_per_barrel', 'fuel_price_inr_per_liter', 'is_holiday',\n",
    "            'holiday_impact_score', 'is_holiday_season'\n",
    "        ]\n",
    "        \n",
    "        # Encoded categorical features\n",
    "        encoded_features = [col for col in df.columns if col.endswith('_encoded')]\n",
    "        \n",
    "        # Combine all features\n",
    "        all_potential_features = numerical_features + encoded_features\n",
    "        \n",
    "        # Select only existing features\n",
    "        for feature in all_potential_features:\n",
    "            if feature in df.columns:\n",
    "                feature_columns.append(feature)\n",
    "        \n",
    "        self.feature_columns = feature_columns\n",
    "        \n",
    "        # Create feature matrix\n",
    "        X = df[feature_columns].copy()\n",
    "        \n",
    "        # Handle any remaining missing values\n",
    "        X = X.fillna(X.median())\n",
    "        \n",
    "        logger.info(f\"Selected {len(feature_columns)} features for modeling\")\n",
    "        return X, feature_columns\n",
    "    \n",
    "    def scale_features(self, X: pd.DataFrame, fit: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Scale features using RobustScaler\"\"\"\n",
    "        if fit:\n",
    "            self.scaler = RobustScaler()\n",
    "            X_scaled = pd.DataFrame(\n",
    "                self.scaler.fit_transform(X),\n",
    "                columns=X.columns,\n",
    "                index=X.index\n",
    "            )\n",
    "            logger.info(\"Fitted and transformed features\")\n",
    "        else:\n",
    "            if self.scaler is None:\n",
    "                raise ValueError(\"Scaler not fitted. Call with fit=True first.\")\n",
    "            X_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X),\n",
    "                columns=X.columns,\n",
    "                index=X.index\n",
    "            )\n",
    "            logger.info(\"Transformed features using existing scaler\")\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def split_data(self, X: pd.DataFrame, y: pd.Series, \n",
    "                   test_size: float = 0.2, val_size: float = 0.2,\n",
    "                   random_state: int = 42) -> Tuple:\n",
    "        \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "        \n",
    "        # First split: train+val and test\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=None\n",
    "        )\n",
    "        \n",
    "        # Second split: train and val\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Data split - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def save_preprocessor(self, filepath: str = \"models/saved/preprocessor.pkl\"):\n",
    "        \"\"\"Save the preprocessor state\"\"\"\n",
    "        preprocessor_state = {\n",
    "            'label_encoders': self.label_encoders,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'target_column': self.target_column\n",
    "        }\n",
    "        \n",
    "        Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(preprocessor_state, filepath)\n",
    "        logger.info(f\"Preprocessor saved to {filepath}\")\n",
    "    \n",
    "    def load_preprocessor(self, filepath: str = \"models/saved/preprocessor.pkl\"):\n",
    "        \"\"\"Load the preprocessor state\"\"\"\n",
    "        preprocessor_state = joblib.load(filepath)\n",
    "        \n",
    "        self.label_encoders = preprocessor_state['label_encoders']\n",
    "        self.scaler = preprocessor_state['scaler']\n",
    "        self.feature_columns = preprocessor_state['feature_columns']\n",
    "        self.target_column = preprocessor_state['target_column']\n",
    "        \n",
    "        logger.info(f\"Preprocessor loaded from {filepath}\")\n",
    "    \n",
    "    def process_full_pipeline(self, data_path: str = \"dataset\") -> Tuple:\n",
    "        \"\"\"Run the complete preprocessing pipeline\"\"\"\n",
    "        logger.info(\"Starting full preprocessing pipeline...\")\n",
    "        \n",
    "        # Load data\n",
    "        df = self.load_airline_data(data_path)\n",
    "        \n",
    "        # Clean data\n",
    "        df_clean = self.clean_data(df)\n",
    "        \n",
    "        # Feature engineering\n",
    "        df_features = self.feature_engineering(df_clean)\n",
    "        \n",
    "        # Encode categorical features\n",
    "        df_encoded = self.encode_categorical_features(df_features)\n",
    "        \n",
    "        # Prepare features for modeling\n",
    "        X, feature_columns = self.prepare_features_for_modeling(df_encoded)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scale_features(X, fit=True)\n",
    "        \n",
    "        # Extract target variable\n",
    "        y = df_encoded[self.target_column]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X_scaled, y)\n",
    "        \n",
    "        # Save preprocessor\n",
    "        self.save_preprocessor()\n",
    "        \n",
    "        # Save processed data\n",
    "        processed_data = {\n",
    "            'X_train': X_train, 'X_val': X_val, 'X_test': X_test,\n",
    "            'y_train': y_train, 'y_val': y_val, 'y_test': y_test,\n",
    "            'feature_columns': feature_columns,\n",
    "            'full_dataset': df_encoded\n",
    "        }\n",
    "        \n",
    "        joblib.dump(processed_data, self.cache_dir / \"processed_data.pkl\")\n",
    "        logger.info(\"Processed data saved\")\n",
    "        \n",
    "        logger.info(\"Full preprocessing pipeline completed successfully!\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test, df_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eedfb64d-5189-48ae-9e0e-9313b5ff53ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting full preprocessing pipeline...\n",
      "INFO:__main__:Loading airline datasets...\n",
      "INFO:__main__:Loaded datasets - Clean: (300153, 12), Business: (93487, 11), Economy: (206774, 11)\n",
      "INFO:__main__:Combined dataset shape: (600414, 16)\n",
      "INFO:__main__:Starting data cleaning...\n",
      "INFO:__main__:Removed 2 duplicate rows\n",
      "INFO:__main__:Removed 246 price outliers\n",
      "INFO:__main__:Data cleaning completed. Final shape: (600166, 16)\n",
      "INFO:__main__:Starting feature engineering...\n",
      "INFO:__main__:Adding external features...\n",
      "INFO:__main__:Enriching flight data with external sources...\n",
      "INFO:__main__:Loading cached fuel data\n",
      "INFO:__main__:Loading cached holiday data\n",
      "INFO:__main__:Enriched flight data with 1 new features\n",
      "INFO:__main__:Feature engineering completed. New shape: (600166, 41)\n",
      "INFO:__main__:Encoding categorical features...\n",
      "INFO:__main__:Preparing features for modeling...\n",
      "INFO:__main__:Selected 33 features for modeling\n",
      "INFO:__main__:Fitted and transformed features\n",
      "INFO:__main__:Data split - Train: (360099, 33), Val: (120033, 33), Test: (120034, 33)\n",
      "INFO:__main__:Preprocessor saved to models/saved/preprocessor.pkl\n",
      "INFO:__main__:Processed data saved\n",
      "INFO:__main__:Full preprocessing pipeline completed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (360099, 33)\n",
      "Validation data shape: (120033, 33)\n",
      "Test data shape: (120034, 33)\n",
      "Feature columns: 33\n",
      "Full dataset shape: (600166, 52)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test the preprocessing pipeline\n",
    "    preprocessor = FlightDataPreprocessor()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, df_full = preprocessor.process_full_pipeline()\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Validation data shape: {X_val.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    print(f\"Feature columns: {len(preprocessor.feature_columns)}\")\n",
    "    print(f\"Full dataset shape: {df_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7c0aa-6635-49ad-8bd0-2af0b16a8dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
